{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoders_MLPR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxZXfBj-ZwXG"
      },
      "source": [
        "# Autoencoders in PyTorch\n",
        "Here, we extend our previous MLP and CNN PyTorch Fashion-MNIST example to the unsupervised learning scenario with autoencoders. \n",
        "\n",
        "Before starting the notebook you should make sure that your runtime uses GPU acceleration. You can find the corresponding option under *runtime* and then *change runtime type*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p-P51_uaXIi"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NB7ZruPuZwXH"
      },
      "source": [
        "### Dataset class in PyTorch\n",
        "Our dataset loader essentially remains the same as before. We do not remove the labels even though we do not need them for training of autoencoders. We will however use a small subset of the labels for semi-supervised training towards the end of the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UhzkhiZZwXI"
      },
      "source": [
        "import os\n",
        "import struct\n",
        "import gzip\n",
        "import errno\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n",
        "class FashionMNIST:\n",
        "    \"\"\"\n",
        "    Fashion MNIST dataset featuring gray-scale 28x28 images of\n",
        "    fashion items belonging to ten different classes.\n",
        "    Dataloader adapted from MNIST.\n",
        "    We do not define __getitem__ and __len__ in this class\n",
        "    as we are using torch.utils.data.TensorDataSet which\n",
        "    already implements these methods.\n",
        "\n",
        "    Parameters:\n",
        "        args (dict): Dictionary of (command line) arguments.\n",
        "            Needs to contain batch_size (int) and workers(int).\n",
        "        is_gpu (bool): True if CUDA is enabled.\n",
        "            Sets value of pin_memory in DataLoader.\n",
        "\n",
        "    Attributes:\n",
        "        trainset (torch.utils.data.TensorDataset): Training set wrapper.\n",
        "        valset (torch.utils.data.TensorDataset): Validation set wrapper.\n",
        "        train_loader (torch.utils.data.DataLoader): Training set loader with shuffling.\n",
        "        val_loader (torch.utils.data.DataLoader): Validation set loader.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_gpu, batch_size, workers):\n",
        "        self.path = os.path.expanduser('datasets/FashionMNIST')\n",
        "        self.__download()\n",
        "\n",
        "        self.trainset, self.valset = self.get_dataset()\n",
        "\n",
        "        self.train_loader, self.val_loader = self.get_dataset_loader(batch_size, workers, is_gpu)\n",
        "\n",
        "        self.val_loader.dataset.class_to_idx = {'T-shirt/top': 0,\n",
        "                                                'Trouser': 1,\n",
        "                                                'Pullover': 2,\n",
        "                                                'Dress': 3,\n",
        "                                                'Coat': 4,\n",
        "                                                'Sandal': 5,\n",
        "                                                'Shirt': 6,\n",
        "                                                'Sneaker': 7,\n",
        "                                                'Bag': 8,\n",
        "                                                'Ankle boot': 9}\n",
        "\n",
        "    def __check_exists(self):\n",
        "        \"\"\"\n",
        "        Checks if dataset has already been downloaded\n",
        "\n",
        "        Returns:\n",
        "             bool: True if downloaded dataset has been found\n",
        "        \"\"\"\n",
        "\n",
        "        return os.path.exists(os.path.join(self.path, 'train-images-idx3-ubyte.gz')) and \\\n",
        "               os.path.exists(os.path.join(self.path, 'train-labels-idx1-ubyte.gz')) and \\\n",
        "               os.path.exists(os.path.join(self.path, 't10k-images-idx3-ubyte.gz')) and \\\n",
        "               os.path.exists(os.path.join(self.path, 't10k-labels-idx1-ubyte.gz'))\n",
        "\n",
        "    def __download(self):\n",
        "        \"\"\"\n",
        "        Downloads the Fashion-MNIST dataset from the web if dataset\n",
        "        hasn't already been downloaded.\n",
        "        \"\"\"\n",
        "\n",
        "        from six.moves import urllib\n",
        "\n",
        "        if self.__check_exists():\n",
        "            return\n",
        "\n",
        "        print(\"Downloading FashionMNIST dataset\")\n",
        "        urls = [\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/train-images-idx3-ubyte.gz',\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/train-labels-idx1-ubyte.gz',\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/t10k-images-idx3-ubyte.gz',\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/t10k-labels-idx1-ubyte.gz',\n",
        "        ]\n",
        "\n",
        "        # download files\n",
        "        try:\n",
        "            os.makedirs(self.path)\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                pass\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        for url in urls:\n",
        "            print('Downloading ' + url)\n",
        "            data = urllib.request.urlopen(url)\n",
        "            filename = url.rpartition('/')[2]\n",
        "            file_path = os.path.join(self.path, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(data.read())\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    def __get_fashion_mnist(self, path, kind='train'):\n",
        "        \"\"\"\n",
        "        Load Fashion-MNIST data\n",
        "\n",
        "        Parameters:\n",
        "            path (str): Base directory path containing .gz files for\n",
        "                the Fashion-MNIST dataset\n",
        "            kind (str): Accepted types are 'train' and 't10k' for\n",
        "                training and validation set stored in .gz files\n",
        "\n",
        "        Returns:\n",
        "            numpy.array: images, labels\n",
        "        \"\"\"\n",
        "\n",
        "        labels_path = os.path.join(path,\n",
        "                                   '%s-labels-idx1-ubyte.gz'\n",
        "                                   % kind)\n",
        "        images_path = os.path.join(path,\n",
        "                                   '%s-images-idx3-ubyte.gz'\n",
        "                                   % kind)\n",
        "\n",
        "        with gzip.open(labels_path, 'rb') as lbpath:\n",
        "            struct.unpack('>II', lbpath.read(8))\n",
        "            labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
        "\n",
        "        with gzip.open(images_path, 'rb') as imgpath:\n",
        "            struct.unpack(\">IIII\", imgpath.read(16))\n",
        "            images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def get_dataset(self):\n",
        "        \"\"\"\n",
        "        Loads and wraps training and validation datasets\n",
        "\n",
        "        Returns:\n",
        "             torch.utils.data.TensorDataset: trainset, valset\n",
        "        \"\"\"\n",
        "\n",
        "        x_train, y_train = self.__get_fashion_mnist(self.path, kind='train')\n",
        "        x_val, y_val = self.__get_fashion_mnist(self.path, kind='t10k')\n",
        "\n",
        "        # This is new with respect to our previous data loader\n",
        "        # convert to torch tensors in range [0, 1]\n",
        "        x_train = torch.from_numpy(x_train).float() / 255\n",
        "        y_train = torch.from_numpy(y_train).long()\n",
        "        x_val = torch.from_numpy(x_val).float() / 255\n",
        "        y_val = torch.from_numpy(y_val).long()\n",
        "\n",
        "        # resize flattened array of images for input to a CNN\n",
        "        # we use the in-place variant of the resize function here\n",
        "        x_train.resize_(x_train.size(0), 1, 28, 28)\n",
        "        x_val.resize_(x_val.size(0), 1, 28, 28)\n",
        "\n",
        "        # TensorDataset wrapper\n",
        "        trainset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "        valset = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "\n",
        "        return trainset, valset\n",
        "\n",
        "    def get_dataset_loader(self, batch_size, workers, is_gpu):\n",
        "        \"\"\"\n",
        "        Defines the dataset loader for wrapped dataset\n",
        "\n",
        "        Parameters:\n",
        "            batch_size (int): Defines the batch size in data loader\n",
        "            workers (int): Number of parallel threads to be used by data loader\n",
        "            is_gpu (bool): True if CUDA is enabled so pin_memory is set to True\n",
        "\n",
        "        Returns:\n",
        "             torch.utils.data.TensorDataset: trainset, valset\n",
        "        \"\"\"\n",
        "\n",
        "        # multi-threaded data loaders\n",
        "        train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True,\n",
        "                                                   num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
        "        test_loader = torch.utils.data.DataLoader(self.valset, batch_size=batch_size, shuffle=True,\n",
        "                                                  num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
        "\n",
        "        return train_loader, test_loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuIk7hjj5Xbl"
      },
      "source": [
        "Let's load the data and set the device to use. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIz2xlhpZwXM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8fd39845-d10a-4e58-d87b-bc4d6b32ebe1"
      },
      "source": [
        "# set a boolean flag that indicates whether a cuda capable GPU is available \n",
        "# we will need this for transferring our tensors to the device and \n",
        "# for persistent memory in the data loader\n",
        "is_gpu = torch.cuda.is_available()\n",
        "print(\"GPU is available:\", is_gpu)\n",
        "print(\"If you are receiving False, try setting your runtime to GPU\")\n",
        "\n",
        "# set the device to cuda if a GPU is available\n",
        "device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n",
        "\n",
        "# in contrast to our MLP from scratch notebook, we need to set the batch size already now\n",
        "# this is because our data loader now requires it.\n",
        "batch_size = 128\n",
        "# we also set the amount of workers, i.e. parallel threads to use in our data loader\n",
        "workers = 4\n",
        "\n",
        "# We can now instantiate our dataset class \n",
        "dataset = FashionMNIST(is_gpu, batch_size, workers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available: True\n",
            "If you are receiving False, try setting your runtime to GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKpmq-3VZwXP"
      },
      "source": [
        "### The MLP based Autoencoder model in PyTorch\n",
        "We extend our 2 hidden layer MLP to an autoencoder by adding a decoder. For convenience we let the decoder architecture (number and amount of units) remain the same as the encoder. Note that however, this is not necessary and could technically be different. \n",
        "\n",
        "Suitable hidden-layer sizes for this task could be 100 and 100, like in our last notebook. \n",
        "Because we are using an optimized GPU implementation, you are welcome and should try larger sizes to see the impact of neural network size (capacity) on our task!\n",
        "\n",
        "From previous weeks we know that there is three ways to build our model: \n",
        "\n",
        "    1. Defining an nn.Sequential() container for encoder and decoder and calling them in the forward function.\n",
        "    2. Defining all the layers and writing an encode and decode function that specify the execution of the individual layers. The encode and decode functions get called in the forward pass.\n",
        "    3. Defining the layers and writing all the calls directly into the forward function. While this is a valid model, we strongly recommend against this, as for more involved models this gets more complicated to read and understand.  \n",
        "    \n",
        "Here we will use option 1 and we will see how to implement version 2 for the variational autoencoder next week."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JXFkMigZwXR"
      },
      "source": [
        "class AE_MLP(nn.Module):\n",
        "    def __init__(self, img_size, latent_dim):\n",
        "        super(AE_MLP, self).__init__()\n",
        "        \n",
        "        self.img_size = img_size\n",
        "        \n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential()\n",
        "        \n",
        "        # decoder\n",
        "        self.decoder = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # You will now have to use a view both to flatten the input\n",
        "        x = x.view(...)\n",
        "        z = self.encoder(...)\n",
        "        x = self.decoder(...)\n",
        "        # because the MLP has flattened, we now need to view the output back as an image\n",
        "        x = x.view(...)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxw9nHKPZwXT"
      },
      "source": [
        "### Defining optimization criterion and optimizer\n",
        "Similarly to our supervised sample we can now use the negative log-likelihood as a loss function and optimize it with a binary cross entropy to measure the reconstruction loss between input and output. \n",
        "\n",
        "As with the CrossEntropy function before PyTorch already implements a Sigmioid function in the BCEWithLogitsLoss and combines it directly in the loss! Alternatively we could just use the BCE loss function and add the sigmoid to the end of our decoder by hand or stick with a mean squared error MSELoss() function.\n",
        "\n",
        "This time we will use an adaptive optimizer that we have learned about in the first lecture, called Adam: https://arxiv.org/abs/1412.6980. A typical learning rate for Adam will be 1e-4 or 1e-3 as the step size is adaptive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHI6LGtuZwXU"
      },
      "source": [
        "# Define optimizer and loss function (criterion)\n",
        "img_size = 28\n",
        "# the latent dimension defines how much the input will be compressed, \n",
        "# this is a hyper-parameter.\n",
        "latent_dim = 20\n",
        "\n",
        "# create an instance of the MLP based autoencoder and transfer the model to the device.\n",
        "# Note that we do not necessarily need any custom weight initialization as PyTorch\n",
        "# already uses the initialization schemes that we have previously learned about internally. \n",
        "model = AE_MLP(img_size, latent_dim).to(device)\n",
        "# we can also print the model architecture\n",
        "print(model)\n",
        "\n",
        "# set the loss function\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# we can use advanced stochastic gradient descent algorithms \n",
        "# with regularization (weight-decay) or momentum\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V40y_5S_epSJ"
      },
      "source": [
        "### Monitoring losses \n",
        "\n",
        "We will reuse our function to monitor loss averages. As we are now conducting unsupervised learning, we no longer have any function calculating accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajy_8OckZwXX"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIqY9nJOZwXa"
      },
      "source": [
        "### Training function\n",
        "The training function needs to loop through the entire dataset in steps of mini-batches (for SGD). For each mini-batch the output of the model and losses are calculated and a *backward* pass is done to calculate gradients and an *optimizer step* is done in order to do the respective update to the model's weights. \n",
        "\n",
        "When the entire dataset has been processed once, one epoch of the training has been conducted. It is common to shuffle the dataset after each epoch. In contrast to our previous notebook from scratch, in this implementation this is handled by the \"sampler\" of the dataset loader. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hyNEtLtZwXa"
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Trains/updates the model for one epoch on the training dataset.\n",
        "\n",
        "    Parameters:\n",
        "        train_loader (torch.utils.data.DataLoader): The trainset dataloader\n",
        "        model (torch.nn.module): Model to be trained\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        optimizer (torch.optim.optimizer): optimizer instance like SGD or Adam\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create an instance of the average meter to track losses\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate through the dataset loader\n",
        "    # we can now discard the labels returned by our old data loader as we no longer need them\n",
        "    for i, (..., ...) in enumerate(train_loader):\n",
        "        # transfer inputs and targets to the GPU (if it is available)\n",
        "        inp = ...\n",
        "        target = ... \n",
        "        \n",
        "        # you can make your autoencoder a denoising autoencoder by \n",
        "        # adding noise to the input, but not to the target!\n",
        "        # To test the denoising autoencoder, uncomment the line below, \n",
        "        # where we add a small Gaussian noise to the original images\n",
        "        #inp = inp + torch.randn(inp.size()).to(device) * 0.1\n",
        "        \n",
        "        # compute output, i.e. the model forward\n",
        "        output = model(...)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(..., ...)\n",
        "\n",
        "        # record loss\n",
        "        losses.update(loss.item(), inp.size(0))\n",
        "\n",
        "        # compute gradient and do the SGD step\n",
        "        # we reset the optimizer with zero_grad to \"flush\" former gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print the loss every 100 mini-batches\n",
        "        if i % 100 == 0:\n",
        "            print('Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(loss=losses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YpNuUY0ZwXd"
      },
      "source": [
        "### Validation function\n",
        "Validation is similar to the training loop, but on a separate dataset with the exception that no update to the weights is performed. This way we can monitor the generalization ability of our model and check whether it is overfitting (memorizing) the training dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zihygz7tZwXd"
      },
      "source": [
        "def validate(val_loader, model, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluates/validates the model\n",
        "\n",
        "    Parameters:\n",
        "        val_loader (torch.utils.data.DataLoader): The validation or testset dataloader\n",
        "        model (torch.nn.module): Model to be evaluated/validated\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create an instance of the average meter to track losses\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode \n",
        "    # (this would be important for e.g. dropout where stochasticity shouldn't be applied during testing)\n",
        "    model.eval()\n",
        "\n",
        "    # avoid computation of gradients and necessary storing of intermediate layer activations\n",
        "    with torch.no_grad():\n",
        "        # iterate through the dataset loader\n",
        "        # we can now discard the labels returned by our old data loader as we no longer need them\n",
        "        for i, (..., ...) in enumerate(val_loader):\n",
        "            # transfer to device\n",
        "            inp = ...\n",
        "            target = ...\n",
        "\n",
        "            # compute output\n",
        "            output = model(...)\n",
        "\n",
        "            # compute loss\n",
        "            loss = criterion(..., ...)\n",
        "\n",
        "            # record loss\n",
        "            losses.update(loss.item(), inp.size(0))\n",
        "            \n",
        "            # visualize only one full mini-batch\n",
        "            if i == (len(val_loader) - 2):\n",
        "                # let us also visualize the last mini-batch of images and reconstructions\n",
        "                # we can use the torchvision utility to make grids of images\n",
        "                print(\"Original images\")\n",
        "                imgs = torchvision.utils.make_grid(inp.cpu(), nrow=int(math.sqrt(inp.size(0))), padding=5)\n",
        "                npimgs = imgs.numpy()\n",
        "                # when using matplotlib the color channels are expected to be in the third \n",
        "                # instead of the first dimension -> tranpose\n",
        "                plt.imshow(np.transpose(npimgs, (1,2,0)))\n",
        "                plt.show()\n",
        "                \n",
        "                # do the same for reconstructions\n",
        "                print(\"Reconstructed images\")\n",
        "                recons = torchvision.utils.make_grid(torch.sigmoid(output).cpu(), nrow=int(math.sqrt(inp.size(0))), padding=5)\n",
        "                npimgs = recons.numpy()\n",
        "                plt.imshow(np.transpose(npimgs, (1,2,0)))\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "    print('Loss {loss.val:.4f} ({loss.avg:.4f})'.format(loss=losses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW-sqbUaZwXi"
      },
      "source": [
        "### Running the training of the model\n",
        "Let's optimize this model for 20 epochs and check at every epoch how we are doing on our validation set. As defined above, we also visualize some images and their reconstructions.\n",
        "\n",
        "Depending on your model definition and optimizer you might experience over-fitting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFvFnowLZwXj"
      },
      "source": [
        "total_epochs = 20\n",
        "for epoch in range(total_epochs):\n",
        "    print(\"EPOCH:\", epoch + 1)\n",
        "    print(\"TRAIN\")\n",
        "    train(...)\n",
        "    print(\"VALIDATION\")\n",
        "    validate(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT7uB1sNk2-e"
      },
      "source": [
        "## From unsupervised learning to semi-supervised learning\n",
        "Now that we have seen that our model was able to successfuly learn how to compress and decompress the data in a completely unsupervised fashion, we can try to make use of the learned representations\n",
        "\n",
        "### Making use of learned representations\n",
        "Assume that we have a dataset where a majority of images is not labelled, so we can not directly use supervised end-to-end learning as encountered in the last weeks. However, our autoencoder's encoder learns a feature description that best described the seen dataset. \n",
        "\n",
        "We can use the feature representation of our autoencoder's encoder and now simply train a classifier (e.g. a simple linear combination) on top with the few labels that we have. This way the classifier can learn how the autoencoder's extracted generic feature representations map to specific classes.\n",
        "\n",
        "For this purpose let us do the following four things:\n",
        "\n",
        "    1. Let us again define an accuracy function to measure our classification progress\n",
        "    2. Let us define supervised training and validation functions, where we now first compute the already trained autoencder's decoder and then compute a classifier\n",
        "    3. Let us define a single layer linear classifier \n",
        "    4. Let us define a new optimizer instance for only the classifier, i.e. the weights of the autoencoder remain fixed.\n",
        "    5. Let us take half of the validation data (i.e. only 5% of the entire datasets) to train the supervised scenario with few labels and test on the remaining 50% of the validation set.\n",
        "\n",
        "### Semi-supervised training and validation functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZFFDlzWnKmc"
      },
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"\n",
        "    Evaluates a model's top k accuracy\n",
        "\n",
        "    Parameters:\n",
        "        output (torch.autograd.Variable): model output\n",
        "        target (torch.autograd.Variable): ground-truths/labels\n",
        "        topk (list): list of integers specifying top-k precisions\n",
        "            to be computed\n",
        "\n",
        "    Returns:\n",
        "        float: percentage of correct predictions\n",
        "    \"\"\"\n",
        "\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0tcFyDimOk-"
      },
      "source": [
        "def train_supervised(train_loader, encoder, img_size, classifier, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Trains/updates the classifier for one epoch on the training dataset.\n",
        "\n",
        "    Parameters:\n",
        "        train_loader (torch.utils.data.DataLoader): The trainset dataloader\n",
        "        encoder (torch.nn.module): Fixed weight encoder\n",
        "        img_size (int): Spatial size of the input images\n",
        "        classifier (torch.nn.module): Classifier to be trained\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        optimizer (torch.optim.optimizer): optimizer instance like SGD or Adam\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create an instance of the average meter to track losses\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    # set the encoder to evaluate mode as it isn't trained \n",
        "    # (this would be important for e.g. dropout where stochasticity shouldn't be applied during testing)\n",
        "    encoder.eval()\n",
        "    classifier.train()\n",
        "\n",
        "    # iterate through the dataset loader\n",
        "    for i, (..., ...) in enumerate(train_loader):\n",
        "        # transfer inputs and targets to the GPU (if it is available)\n",
        "        inp = ...\n",
        "        target = ...\n",
        "        \n",
        "        # compute output, i.e. the model forward\n",
        "        # first flatten the image and calculate the static autoencoder's encoder\n",
        "        inp = inp.view(...)\n",
        "        embedding = encoder(...)\n",
        "        # then compute the classifier on the obtained embedding\n",
        "        output = classifier(...)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(..., ...)\n",
        "\n",
        "        # measure accuracy and record loss and accuracy\n",
        "        prec1, _ = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), inp.size(0))\n",
        "        top1.update(prec1.item(), inp.size(0))\n",
        "\n",
        "        # compute gradient and do the SGD step\n",
        "        # we reset the optimizer with zero_grad to \"flush\" former gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print the loss every 100 mini-batches\n",
        "        if i % 100 == 0:\n",
        "            print('Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                   loss=losses, top1=top1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZBmEls7niIA"
      },
      "source": [
        "def validate_supervised(val_loader, encoder, img_size, classifier, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model\n",
        "\n",
        "    Parameters:\n",
        "        val_loader (torch.utils.data.DataLoader): The valset dataloader\n",
        "        encoder (torch.nn.module): Fixed weight encoder\n",
        "        img_size (int): Spatial size of the input images\n",
        "        classifier (torch.nn.module): Classifier to be trained\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create instances of the average meter to track losses and accuracies\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode \n",
        "    # (this would be important for e.g. dropout where stochasticity shouldn't be applied during testing)\n",
        "    encoder.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    # avoid computation of gradients and necessary storing of intermediate layer activations\n",
        "    with torch.no_grad():\n",
        "        # iterate through the dataset loader\n",
        "        for i, (..., ...) in enumerate(val_loader):\n",
        "            # transfer to device\n",
        "            inp = ...\n",
        "            target = ...\n",
        "\n",
        "            # compute output, i.e. the model forward\n",
        "            # first flatten the image and calculate the static autoencoder's encoder\n",
        "            inp = inp.view(...)\n",
        "            embedding = encoder(...)\n",
        "            # then compute the classifier on the obtained embedding\n",
        "            output = classifier(...)\n",
        "\n",
        "            # compute loss\n",
        "            loss = criterion(..., ...)\n",
        "\n",
        "            # measure accuracy and record loss and accuracy\n",
        "            prec1, _ = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), inp.size(0))\n",
        "            top1.update(prec1.item(), inp.size(0))\n",
        "\n",
        "    print(' * Validation accuracy: Prec@1 {top1.avg:.3f} '.format(top1=top1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS9ZlIA8vUTB"
      },
      "source": [
        "#### Semi-supervised optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrGYI62lk3Ko"
      },
      "source": [
        "n_classes = len(dataset.val_loader.dataset.class_to_idx)\n",
        "\n",
        "# Define the single linear layer classifier that maps from the encoder's latent \n",
        "# embedding to the amount of classes. \n",
        "classifier = ...\n",
        "\n",
        "# Let us split the validation dataset into two separate sets, each containing \n",
        "# 50% of the data to use for supervised learning and validation. \n",
        "# We can use the torch.utils.data.random_split function for this purpose.\n",
        "labeled_train_set, labeled_val_set = torch.utils.data.random_split(dataset.valset, [int(0.5 * len(dataset.valset)), int(0.5 * len(dataset.valset))])\n",
        "supervised_train_loader = torch.utils.data.DataLoader(labeled_train_set, batch_size=batch_size, shuffle=True, \n",
        "                                                      num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
        "supervised_test_loader = torch.utils.data.DataLoader(labeled_val_set, batch_size=batch_size, shuffle=False, \n",
        "                                                     num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
        "\n",
        "# set the supervised loss function\n",
        "criterion = ...\n",
        "\n",
        "# create the optimizer. \n",
        "# IMPORTANT: we will only optimize the classifier parameters here and keep \n",
        "# the encoder parameters fixed to see how far we can get just by fine-tuning\n",
        "# on a few labels. \n",
        "optimizer = torch.optim.Adam(...)\n",
        "\n",
        "total_epochs = 40\n",
        "for epoch in range(total_epochs):\n",
        "    print(\"EPOCH:\", epoch + 1)\n",
        "    print(\"TRAIN\")\n",
        "    train_supervised(...)\n",
        "    print(\"VALIDATION\")\n",
        "    validate_supervised(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCAupFP6sgWg"
      },
      "source": [
        "### The pre-training clearly helped us to use only few labels to map the existing encoding to class concepts. However, our results seem to be much worse than what we likely expected. On the other hand, we have just used 5% of the labels of the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "kldGeVaXZwXo"
      },
      "source": [
        "## Moving from MLP to CNN\n",
        "We will now implement the autoencoder for our previously used convolutional neural network. For downsampling operations such as pooling, we can analogously implement upsampling in the decoder. For strided convolution, we could add stride to the transposed version.\n",
        "\n",
        "As previously mentioned we will implement the model through individual functions instead of nn.Sequential containers now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RK8xT8LhUnV"
      },
      "source": [
        "class AE_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AE_CNN, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(nn.Conv2d(1, 64, 5), # input features, output features, kernel size \n",
        "                                     nn.ReLU(True), \n",
        "                                     nn.MaxPool2d(2, 2), # kernel size, stride \n",
        "                                     nn.Conv2d(64, 128, 5), # input features, output features, kernel size \n",
        "                                     nn.ReLU(True), \n",
        "                                     nn.MaxPool2d(2, 2)) # kernel size, stride\n",
        "    \n",
        "        # You can use the function nn.ConvTranspose2d() for the transposed convolutions\n",
        "        # in the decoder. \n",
        "        \n",
        "        # You can use the function nn.Upsample(scale_factor=) to upsample as the \n",
        "        # function to compensate the downsampling through pooling in the encoder.\n",
        "\n",
        "        # Build a symmetric decoder \n",
        "        self.decoder = nn.Sequential(...)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        x = self.decoder(h)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjaJ3IxciHSD"
      },
      "source": [
        "# create an instance of the MLP based autoencoder and transfer the model to the device.\n",
        "# Note that we do not necessarily need any custom weight initialization as PyTorch\n",
        "# already uses the initialization schemes that we have previously learned about internally. \n",
        "CNN_AE_model = AE_CNN().to(device)\n",
        "# we can also print the model architecture\n",
        "print(CNN_AE_model)\n",
        "\n",
        "# set the loss function\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# we can use advanced stochastic gradient descent algorithms \n",
        "# with regularization (weight-decay) or momentum\n",
        "optimizer = torch.optim.Adam(CNN_AE_model.parameters(), lr=1e-4)\n",
        "\n",
        "total_epochs = 20\n",
        "for epoch in range(total_epochs):\n",
        "    print(\"EPOCH:\", epoch + 1)\n",
        "    print(\"TRAIN\")\n",
        "    train(...)\n",
        "    print(\"VALIDATION\")\n",
        "    validate(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjqGVc8MlOx0"
      },
      "source": [
        "We can observe that already our first epoch achieves a better loss and qualitatively better looking reconstructed images in the convolutional neural network autoencoder variant.\n",
        "\n",
        "As this encoding seems to be much better, we can ask ourselves the question whether our semi-supervised learning approach will fare much better now. This is something that you should try to get a better feeling for the learned representations. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0hu95n8n6D"
      },
      "source": [
        "# Variational Autoencoder\n",
        "We have seen how we can encode data into a latent vector and decode back to the original image for training with reconstruction loss.  \n",
        "\n",
        "In the lecture we have encountered another Bayesian variant, called the variational autoencoder that provides a different perspective with a powerful generative model: https://arxiv.org/abs/1312.6114 . \n",
        "We will now minimize the variational lower bound on the evidence (ELBO). To approximate the true posterior to the generative model p(x, z) we will use our neural network encoder to calculate q(x|z) (the approximate posterior). This is sometimes also referred to as the recognition model.\n",
        "\n",
        "The probabilistic decoder p(x|z) in turn computes the probability density of an input x under the generative model given a sample z from the approximate posterior q(z|x). Like in the autoencoder the parameters of encoder and decoder are optimized jointly. \n",
        "\n",
        "We can conduct this optimization using the reparameterization trick, where we express the random variable z through a deterministic variable as seen in the lecture.\n",
        "\n",
        "In practical terms, in contrast to the regular autoencoder, we thus now optimize for both a reconstruction loss and KL divergence between our prior and approximate posterior. \n",
        "\n",
        "Following the lecture, we will use a Gaussian prior and implement the corresponding reparameterization.\n",
        "\n",
        "A good in-depth introduction to variational auto-encoders for further reading can be found here: https://arxiv.org/abs/1906.02691\n",
        "\n",
        "## Building our CNN model\n",
        "Last week we have implemented our PyTorch models with nn.Sequential() containers in order to easily access the model.encoder to train a classifier on top. Prior to that we have seen how we can define our model's layers in the class' constructor and use a forward function to define the execution. This week we will encounter another third variant, where we define multiple custom functions called encode, decode and reparameterize which we can then individually access and define a joint forward function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynFvH-VJ8ojI"
      },
      "source": [
        "class VAE_CNN(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VAE_CNN, self).__init__()\n",
        "        \n",
        "        self.latent_dim = latent_dim \n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 64, 5) # input features, output features, kernel size\n",
        "        self.mp1 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(64, 128, 5) # input features, output features, kernel size\n",
        "        self.mp2 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
        "        \n",
        "        # tip: 4x4 is the remaining spatial resolution here\n",
        "        self.latent_mu = ...\n",
        "        self.latent_sigma = ...\n",
        "        \n",
        "        # decoder layers\n",
        "        # implement the decoder layers \n",
        "        ...\n",
        "        self.Upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        \n",
        "    def encode(self, x):\n",
        "        # define the probabilistic encoding\n",
        "        ...\n",
        "        return mu, sigma\n",
        "        \n",
        "    def reparameterize(self, mu, std):\n",
        "        # define the reparameterization: z = eps * sigma + mu\n",
        "        # where eps ~ N(0, 1)\n",
        "        ...\n",
        "        return z\n",
        "    \n",
        "    def decode(self, z):\n",
        "        # define the probabilistic decoding\n",
        "        ...\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # define the end-to-end forward function\n",
        "        ...\n",
        "\n",
        "        # make sure to return output, was well as latent mu and sigma to \n",
        "        # calculate the reconstruction and KLD loss terms\n",
        "        return x, mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74k0_BFb8trD"
      },
      "source": [
        "#### We will now also have to define our loss function\n",
        "In addition to the reconstruction loss, we need to implement the KL divergence between the approximate posterior, thus our latent mu and sigma and our unit Gaussian prior. \n",
        "\n",
        "We extend our loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F11vGKO8tIs"
      },
      "source": [
        "def VAE_loss_function(output, target, mu, std):\n",
        "    recon_loss_func = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "    # compute reconstruction loss - normalize by batch size\n",
        "    recon_loss = ...\n",
        "    \n",
        "    # numerical value for stability of log computation\n",
        "    eps = 1e-8\n",
        "    # compute Kullback Leibler Divergence - normalize by batch size\n",
        "    kld = ...\n",
        "    \n",
        "    return recon_loss, kld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShyoOkfV8zPc"
      },
      "source": [
        "We will also need to modify our training and validation functions to include the modified loss and let the model return our latent mu and sigma vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbXuTX98zqh"
      },
      "source": [
        "def train_VAE(train_loader, model, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Trains/updates the model for one epoch on the training dataset.\n",
        "\n",
        "    Parameters:\n",
        "        train_loader (torch.utils.data.DataLoader): The trainset dataloader\n",
        "        model (torch.nn.module): Model to be trained\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        optimizer (torch.optim.optimizer): optimizer instance like SGD or Adam\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create an instance of the average meter to track losses\n",
        "    losses = AverageMeter()\n",
        "    recon_losses = AverageMeter()\n",
        "    kl_losses = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate through the dataset loader\n",
        "    # we can now discard the labels returned by our old data loader as we no longer need them\n",
        "    for i, (inp, _) in enumerate(train_loader):\n",
        "        # transfer inputs and targets to the GPU (if it is available)\n",
        "        inp = inp.to(device)\n",
        "        target = inp\n",
        "        \n",
        "        # you can make your autoencoder a denoising autoencoder by \n",
        "        # adding noise to the input, but not to the target!\n",
        "        # To test the denoising autoencoder, uncomment the line below, \n",
        "        # where we add a small Gaussian noise to the original images\n",
        "        #inp = inp + torch.randn(inp.size()).to(device) * 0.1\n",
        "        \n",
        "        # compute output, i.e. the model forward. \n",
        "        # In contrast to our autoencoder this now also returns the latent mu\n",
        "        # and sigma that we need to calculate the KL divergence\n",
        "        ... = model(...)\n",
        "        \n",
        "        # calculate the loss\n",
        "        recon_loss, kl_loss = criterion(...)\n",
        "        loss = ...\n",
        "\n",
        "        # record loss\n",
        "        losses.update(loss.item(), inp.size(0))\n",
        "        recon_losses.update(recon_loss.item(), inp.size(0))\n",
        "        kl_losses.update(kl_loss.item(), inp.size(0))\n",
        "\n",
        "        # compute gradient and do the SGD step\n",
        "        # we reset the optimizer with zero_grad to \"flush\" former gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print the loss every 100 mini-batches\n",
        "        if i % 100 == 0:\n",
        "            print('Loss {loss.val:.4f} ({loss.avg:.4f}) \\t'\n",
        "                  'Reconstruction {recon.val:.4f} ({recon.avg:.4f}) \\t'\n",
        "                  'KLD {kld.val:.4f} ({kld.avg:.4f})'\n",
        "                  .format(loss=losses, recon=recon_losses, kld=kl_losses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHdQSe4282cq"
      },
      "source": [
        "def validate_VAE(val_loader, model, criterion, device):\n",
        "    \"\"\"\n",
        "    Trains/updates the model for one epoch on the training dataset.\n",
        "\n",
        "    Parameters:\n",
        "        val_loader (torch.utils.data.DataLoader): The valset dataloader\n",
        "        model (torch.nn.module): Model to be trained\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create an instance of the average meter to track losses\n",
        "    losses = AverageMeter()\n",
        "    recon_losses = AverageMeter()\n",
        "    kl_losses = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.eval()\n",
        "\n",
        "    # iterate through the dataset loader\n",
        "    # we can now discard the labels returned by our old data loader as we no longer need them\n",
        "    for i, (inp, _) in enumerate(val_loader):\n",
        "        # transfer inputs and targets to the GPU (if it is available)\n",
        "        inp = inp.to(device)\n",
        "        target = inp\n",
        "        \n",
        "        # compute output, i.e. the model forward. \n",
        "        # In contrast to our autoencoder this now also returns the latent mu\n",
        "        # and sigma that we need to calculate the KL divergence\n",
        "        ... = model(...)\n",
        "        \n",
        "        # calculate the loss\n",
        "        recon_loss, kl_loss = criterion(...)\n",
        "        loss = ...\n",
        "\n",
        "        # record loss\n",
        "        losses.update(loss.item(), inp.size(0))\n",
        "        recon_losses.update(recon_loss.item(), inp.size(0))\n",
        "        kl_losses.update(kl_loss.item(), inp.size(0))\n",
        "        \n",
        "        # print the loss every 100 mini-batches\n",
        "        if i % 100 == 0:\n",
        "            print('Loss {loss.val:.4f} ({loss.avg:.4f}) \\t'\n",
        "                  'Reconstruction {recon.val:.4f} ({recon.avg:.4f}) \\t'\n",
        "                  'KLD {kld.val:.4f} ({kld.avg:.4f})'\n",
        "                  .format(loss=losses, recon=recon_losses, kld=kl_losses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PONKpu6m8426"
      },
      "source": [
        "In addition to training and validation, we can now also write a generation function. Once the model is training and minimizing the divergence between our approximate posterior distribution and our Gaussian prior, we can start sampling from the prior and directly generate images from the corresponding latent vector z. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucK8qPpQ867p"
      },
      "source": [
        "def generate(model, device, num_images=100):\n",
        "    with torch.no_grad():\n",
        "        # sample from the Gaussian prior p(z)\n",
        "        z = ...\n",
        "\n",
        "        # decode the samples p(x|z)\n",
        "        # add a sigmoid function at the end of the decoder to constrain \n",
        "        # the generated image to 0-1 range (which is otherwise not guaranteed)\n",
        "        generated_images = ...\n",
        "\n",
        "        # visualize\n",
        "        imgs = torchvision.utils.make_grid(generated_images.cpu(),\n",
        "                                           nrow=int(math.sqrt(generated_images.size(0))),\n",
        "                                           padding=5)\n",
        "        npimgs = imgs.numpy()\n",
        "        # when using matplotlib the color channels are expected to be in the third \n",
        "        # instead of the first dimension -> tranpose\n",
        "        plt.imshow(np.transpose(npimgs, (1,2,0)))\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhH-JUwz89_-"
      },
      "source": [
        "### Constructing and running the CNN based VAE\n",
        "Let's create an instance of our VAE model and optimize it. After each epoch we will sample from the prior and generate some example images and took a look at their visual quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJeFk5438-bQ"
      },
      "source": [
        "# create VAE model instance\n",
        "latent_dim = 50 # like in the autoencoder, this is a hyper-parameter\n",
        "# a good starting point for the above model is a latent dimension of 50.\n",
        "model = VAE_CNN(latent_dim).to(device)\n",
        "print(model)\n",
        "\n",
        "# again, define loss function and optimizer. This time using our custom loss.\n",
        "criterion = VAE_loss_function\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# optimize and visualize generations from generative model at each epoch\n",
        "total_epochs = 20\n",
        "for epoch in range(total_epochs):\n",
        "    print(\"EPOCH:\", epoch + 1)\n",
        "    print(\"TRAIN\")\n",
        "    train_VAE(dataset.train_loader, model, criterion, optimizer, device)\n",
        "    print(\"VALIDATION\")\n",
        "    validate_VAE(dataset.val_loader, model, criterion, device)\n",
        "    print(\"GENERATION\")\n",
        "    generate(model, device, num_images=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f5E8YiT9Bsp"
      },
      "source": [
        "We can see that initially the samples do not look like the data from our training distribution, but after some epochs of optimizing the generative model we are able to generate images by decoding the samples from our Gaussian prior. \n",
        "\n",
        "# Additional (optional) exercise\n",
        "\n",
        "Use a 2 dimensional latent space and visualize the latent space of the trained model as well as traverse deterministic z values during generation to see how the model interpolates between different concepts. "
      ]
    }
  ]
}