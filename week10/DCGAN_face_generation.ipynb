{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_face_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcCgyQu-CsM8",
        "colab_type": "text"
      },
      "source": [
        "# Generative Adversarial Networks (GANs)\n",
        "In our previous lecture on unsupervised neural networks and generative models we have encountered one generative model, namely the vartiational autoencoder (VAE), and only briefly outlined generative adversarial networks (GANs). Due to the immense popularity of the method, we are now coming back to the latter.\n",
        "\n",
        "We will get to know GANs on a dataset containing celebrity faces called CelebA: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html that was first introduced at ICCV2015. \n",
        "\n",
        "The code in this notebook is heavily inspired, adapted and partially taken from the great PyTorch beginner tutorial on GANs: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "\n",
        "## GANs\n",
        "GANs were originally proposed by Ian Goodfellow et. al in their analogously called NeurIPS 2014 paper: https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf \n",
        "\n",
        "The general idea behind them is that we have two neural network models. On the one hand, we have a Generator G that is trying to generate images from a randomly sampled latent code z. On the other hand, we have a Discriminator D that is conducting a binary classification between images from a real dataset and the images generated by G. \n",
        "\n",
        "The idea behind training is to play a min-max game by training the discriminator to separate real from fake images and at the same time training the generator to fool the discriminator into believing its images are real. In other words,  if D(G(z)) is the probability that a generated image is a real image and D(x) the probability that an image from the dataset is considered as real, then the following loss function is to be optimized: \n",
        "\n",
        "$min_G max_D V(D,G)=ùîº_{x \\sim p_{data}(x)}\\left[ \\log{D(x)} \\right]+ùîº_{z \\sim p_{z}(z)} \\left[\\log{(1‚àíD(G(z)))} \\right]$\n",
        "\n",
        "Here, G tries to minimize the probability that D predicts its output as fake (as indicated by the 1 - term) and D tries to maximize its probability of separating real from fake images correctly. \n",
        "\n",
        "After training, the ideal outcome would be that the generator produces images that are able to fool the discriminator into randomly guessing whether an image is real or fake. However, GANs are notoriously known to be unstable during training and much of the currenct research is focused on preventing collapse, smooth training and robustness to hyperparameters.\n",
        "\n",
        "## Architectural similarity to VAE\n",
        "\n",
        "We can see an architectural analogy of GANs to VAEs as the two separate architectural components are trained jointly in a VAE (although there is many more mathematical differences of course). The decoder in a VAE is essentially analogous to the GAN's generator that tries to capture the data distribution, draw samples from a latent space and generate an image. The VAE's encoder is architecturally similar to the GAN's discriminator, although the task here of course is to classify fake vs. real instead of approximating the posterior. \n",
        "\n",
        "Much of the newer research bridges GANs and VAEs and is creating hybrid models that combine the advantages of both methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cfBfJBWEzzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d643a102-b5b7-4b92-98e7-cd0a064174a8"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "import random\n",
        "manualSeed = 999\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f436a97afd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrvpKs8sFHs5",
        "colab_type": "text"
      },
      "source": [
        "## CelebA: celebrity face dataset\n",
        "\n",
        "We will first download, and make the CelebA dataset ready: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html  for our use with PyTorch. \n",
        "\n",
        "If the recommended Dropbox download doesn't work we will resort to a Google Drive download script from the following GitHub gist: \n",
        "https://gist.github.com/charlesreid1/4f3d676b33b95fce83af08e4ec261822 . Alternatively you can download it by hand and upload it to Colab. \n",
        "\n",
        "We then proceed to extract the downloaded zip file and do some formatting to be usable with PyTorch dataloaders. \n",
        "Specifically, as we will be using PyTorch's \"ImageFolder\" dataloader that loads images from subdirectories, we extract the dataset into a subdirectory called \"data/celebA\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPgfZZqVCk0L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "a91bc37d-3fbb-4314-f509-a6f13d36227a"
      },
      "source": [
        "# CelebA images\n",
        "!mkdir -p ./data/\n",
        "if not os.path.exists('./data/celeba.zip'):\n",
        "    !wget -N https://www.dropbox.com/s/d1kjpkqklf0uw77/celeba.zip?dl=0 -O ./data/celeba.zip\n",
        "    !unzip -qq ./data/celeba.zip -d ./data/\n",
        "\n",
        "\"\"\"\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "# TAKE ID FROM SHAREABLE LINK\n",
        "file_id = \"0B7EVK8r0v71pZjFTYXZWM3FlRnM\"\n",
        "# DESTINATION FILE ON YOUR DISK\n",
        "file_path = \"celebA.zip\"\n",
        "print(\"Downloading...\")\n",
        "download_file_from_google_drive(file_id, file_path)\n",
        "print(\"Download successful\")\n",
        "\n",
        "save_root = 'data/'\n",
        "if not os.path.isdir(save_root):\n",
        "    os.mkdir(save_root)\n",
        "\n",
        "print(\"Extracting zip...\")\n",
        "with zipfile.ZipFile(file_path) as zf:\n",
        "    data_dir = zf.namelist()[0]\n",
        "    zf.extractall(save_root)\n",
        "    os.remove(file_path)\n",
        "print(\"Extraction successful\")\n",
        "\n",
        "!mv data/img_align_celeba data/celebA\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: timestamping does nothing in combination with -O. See the manual\n",
            "for details.\n",
            "\n",
            "--2020-01-12 18:17:52--  https://www.dropbox.com/s/d1kjpkqklf0uw77/celeba.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/d1kjpkqklf0uw77/celeba.zip [following]\n",
            "--2020-01-12 18:17:52--  https://www.dropbox.com/s/raw/d1kjpkqklf0uw77/celeba.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc22cbb58c60e29bb364f900962d.dl.dropboxusercontent.com/cd/0/inline/AwAL6T_In04XH0mQWaMaBgJQhNC5y54jMI7UjtFxjUBd9A9777hsz8Ucg8UfJDtcpKtPzAC_EgZmsI9DePXRuWf2myMJ1r11gkBAOupbqkmGQ5J3agxzNv0ftrrHaO0jhts/file# [following]\n",
            "--2020-01-12 18:17:52--  https://uc22cbb58c60e29bb364f900962d.dl.dropboxusercontent.com/cd/0/inline/AwAL6T_In04XH0mQWaMaBgJQhNC5y54jMI7UjtFxjUBd9A9777hsz8Ucg8UfJDtcpKtPzAC_EgZmsI9DePXRuWf2myMJ1r11gkBAOupbqkmGQ5J3agxzNv0ftrrHaO0jhts/file\n",
            "Resolving uc22cbb58c60e29bb364f900962d.dl.dropboxusercontent.com (uc22cbb58c60e29bb364f900962d.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to uc22cbb58c60e29bb364f900962d.dl.dropboxusercontent.com (uc22cbb58c60e29bb364f900962d.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AwAbYUgh7TaQgDuwqgSMDlRDDKsRhX5QPGA97tuJAT7SBtNYfVWATJ1gSjqcb--wOBRc3tu1lOyiNzRd1ZdSmcaWl7DO5kXYLntkyJP4RNaMxLRLaRmUdRQ689jwtnGKvEe_qpxCMsNfsF1FGj-vb9vcXZMVW4pOCOMV8VGvpMN1dU5caT_uZ3hjW-jBgklaRGtZNOhbaaiDA68OqJJM49XE4C1vFwLx3GDZ5UH-QAAVRlmhvWOBwfxsJRy4t6LSUC8adzPb_DelTdpWLlX5ei1l_YaZ4JFgduxwSpTwlLCFGIOTzWnFwmYHtg5xJt0N4LeC4MArBUGnVNnbRazYD9_ONVBDxtUceadkikF0IzxUug/file [following]\n",
            "--2020-01-12 18:17:53--  https://uc22cbb58c60e29bb364f900962d.dl.dropboxusercontent.com/cd/0/inline2/AwAbYUgh7TaQgDuwqgSMDlRDDKsRhX5QPGA97tuJAT7SBtNYfVWATJ1gSjqcb--wOBRc3tu1lOyiNzRd1ZdSmcaWl7DO5kXYLntkyJP4RNaMxLRLaRmUdRQ689jwtnGKvEe_qpxCMsNfsF1FGj-vb9vcXZMVW4pOCOMV8VGvpMN1dU5caT_uZ3hjW-jBgklaRGtZNOhbaaiDA68OqJJM49XE4C1vFwLx3GDZ5UH-QAAVRlmhvWOBwfxsJRy4t6LSUC8adzPb_DelTdpWLlX5ei1l_YaZ4JFgduxwSpTwlLCFGIOTzWnFwmYHtg5xJt0N4LeC4MArBUGnVNnbRazYD9_ONVBDxtUceadkikF0IzxUug/file\n",
            "Reusing existing connection to uc22cbb58c60e29bb364f900962d.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1417753833 (1.3G) [application/zip]\n",
            "Saving to: ‚Äò./data/celeba.zip‚Äô\n",
            "\n",
            "./data/celeba.zip    22%[===>                ] 297.99M  5.45MB/s    eta 2m 22s "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox9-hDBGEy1d",
        "colab_type": "text"
      },
      "source": [
        "Once the dataset is downloaded, extracted and the needed subdirectory is created, we can use PyTorch's image loading functionality to get the convenient dataset loader. \n",
        "\n",
        "Note that the images aren't square in size, so we will employ a common trick and resize all images' lower spatial dimension to 64 (with the other dimension being flexible and always larger) and then take center square crops of size 64 x 64 when training our neural network. Although we might lose some information in this process this prevents introduction of image distortions. As our images always contain faces at the center, the crop isn't much of an issue, but we could technically further introduce a dataset augmentation effect if we were to randomize these crops during training.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmFJtEV-FZV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = 64 \n",
        "dataset = torchvision.datasets.ImageFolder('./data/celeba', \n",
        "                                           transform=transforms.Compose([\n",
        "                                               transforms.Resize(image_size), \n",
        "                                               transforms.CenterCrop(image_size),\n",
        "                                               transforms.ToTensor(), \n",
        "                                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                           ]))\n",
        "\n",
        "# Create the dataloader\n",
        "batch_size = 128\n",
        "workers = 4\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch[0].to(device)[:64],\n",
        "                                                    padding=5, normalize=True).cpu(),\n",
        "                        (1, 2, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVYwSZtZ1-NO",
        "colab_type": "text"
      },
      "source": [
        "## Our GAN's architecture: DCGAN\n",
        "\n",
        "There has been follow-up work on the original GAN paper that improves generation by using convolutional neural networks and applies a couple of additional tricks to make training smoother. As we are already familiar with CNNs we will directly proceed with such a variant of GANs referred to as DCGAN: https://arxiv.org/pdf/1511.06434.pdf . \n",
        "\n",
        "The main differences to the original paper are:\n",
        "\n",
        "* use of convolutions in Generator G and Discriminator D\n",
        "* inclusion of batch normalization layers after activation functions that allow for more stable training. You can read up on batch normalization in the respective paper: https://arxiv.org/abs/1502.03167\n",
        "* LeakyReLUs in the discriminator that have an additional negative slope for values below 0: https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU \n",
        "* Strided convolutions instead of pooling so that the network can learn more complex down or upsampling behavior by itself\n",
        "\n",
        "The architecture of G and D are illustrated nicely in the following diagram. We will need to implement the code for G and D below.\n",
        "\n",
        "![](https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN/blob/master/pytorch_DCGAN.png?raw=true) (figure from: https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN/blob/master/pytorch_DCGAN.png?raw=true)\n",
        "\n",
        "Another crucial contribution of the DCGAN paper is the proposal of hyper-parameters that are very relevant to the stability of training. This specifically includes:\n",
        "\n",
        "* normalization of the data to the range -1 and +1 (as already implemented in above data loader transformations). Correspondingly a tanh function is used at the output of the generator. \n",
        "* a specific weight initialization scheme from a Normal distribution with mean=0 and standard deviation=0.02 for all layers. This is fundamentally different from the weight initialization techniques that we have previously encounterd that typically scale the standard deviation based on a layer's width. \n",
        "\n",
        "**To speed up the computation we will make all layers half as wide as specified in above graph and the original DCGAN. This way we can get results in a reasonable amount of time that still look appealing enough. As we are nearing the end of this course, this is a good exercise to learn how to implAement something according to the discreption provided in a paper. Let's implement G and D.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh9xNInpl98s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement the Generator here according to the architecture diagram above.\n",
        "\n",
        "# Use padding = 0 (the fifth argument of ConvTranspose2d) in the first \n",
        "# transposed convolution (ConvTranspose2d) and 1 in all others \n",
        "\n",
        "# Remember that batch norm comes before the ReLU\n",
        "\n",
        "# for debugging purposes the state sizes are indicated at each block\n",
        "\n",
        "# set the bias=False everywhere. \n",
        "\n",
        "# Build the network using a sequential container. Simply specify the layer\n",
        "# and add the next one, separated by a comma. You don't need to change the \n",
        "# forward function\n",
        "\n",
        "# don't forget the tanh\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        self.gen = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            \n",
        "            \n",
        "            # first block here\n",
        "            # state size: 512 x 4 x 4\n",
        "            \n",
        "            # second block here\n",
        "            # state size: 256 x 8 x 8\n",
        "            \n",
        "            # third block here\n",
        "            # state size: 128 x 16 x 16\n",
        "            \n",
        "            # fourth block\n",
        "            # state size: 64 x 32 x 32\n",
        "            \n",
        "            # last block, ending on an image\n",
        "            # state size: 3 x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.gen(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoPbjrGsmWYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the discriminator according to the above diagram. \n",
        "\n",
        "# Remember the subletly that the first block is an embedding and doesn't have \n",
        "# a batch norm layer. \n",
        "\n",
        "# use the LeakyReLU as indicated above. The PyTorch syntax is \n",
        "# nn.LeakyRelu(value, inplace=True), where value is the negative slope.\n",
        "\n",
        "# Build the network using a sequential container. Simply specify the layer\n",
        "# and add the next one, separated by a comma. You don't need to change the \n",
        "# forward function\n",
        "\n",
        "# don't forget the Sigmoid\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input is 64 x 64 x 64\n",
        "            # first block\n",
        "            # state size: 64 x 32 x 32\n",
        "            \n",
        "            # second block\n",
        "            # state size: 128 x 16 x 16\n",
        "            \n",
        "            # third block\n",
        "            # state size: 256 x 8 x 8\n",
        "            \n",
        "            # fourth block\n",
        "            # state size. 512 x 4 x 4\n",
        "            \n",
        "            # last block ending on classifier\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.disc(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOlop0DFmWie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# our custom weights initialization that we need to call on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6beObNAGKa6",
        "colab_type": "text"
      },
      "source": [
        "Create our generator and discriminator and initialize them with our custom weight initialization function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7kKxLbfmlc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6a809868-1acd-4b1c-deb6-236744ce9965"
      },
      "source": [
        "# Create the generator with a latent dimensionality of 100\n",
        "netG = Generator(100).to(device)\n",
        "\n",
        "# Create the Discriminator\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "# Apply the weights_init function to both networks\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the GAN\n",
        "print(netG)\n",
        "print(netD)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (gen): Sequential()\n",
            ")\n",
            "Discriminator(\n",
            "  (disc): Sequential()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObysealE2MCJ",
        "colab_type": "text"
      },
      "source": [
        "## Training the GAN\n",
        "\n",
        "We already know a suitable loss function to optimize the networks' log likelihoods: the binary cross entropy loss (called BCELoss in PyTorch.\n",
        "\n",
        "$‚Ñì(x,y) = L = \\{l_1,‚Ä¶,l_N\\}^{T}, \\quad l_n = ‚àí\\left[y_n \\log{x_n}+(1‚àíy_n) \\log{(1‚àíx_n)}\\right]$\n",
        "\n",
        "If we set our labels correspondingly we can use the same function for both G and D as the loss already contains a 1 minus term. The convention that is typically used for this is to set the labels of the real images to 1 and those of the fake images to 0. \n",
        "\n",
        "Once we have done this, we can create the two optimizers for the parameters of G and D respectively. \n",
        "\n",
        "To monitor our training progress we also sample a *fixed* mini-batch of latent vectors. We can use this fixed noise to generate images using G at different stages of training to visually gauge our progress. Note that this is not a formally correct way to estimate how good a GAN is performing. There is several metrics that have been proposed in the literature to gauge the quality of GAN. However, as we cannot simply compute the log-likelihood of the data as with the VAE, there is much research that is still being conducted in this direction and it is very much an open research question. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH6jXsDgm_m6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "c961fab2-7a76-4b49-fdd5-26d28d2c18d0"
      },
      "source": [
        "# Learning rate for the optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Sample a mini-batch latent vectors that we will use to \n",
        "# visualize the progression of the generator\n",
        "fixed_noise = torch.randn(64, netG.latent_dim, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-273eb660bffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Setup Adam optimizers for both G and D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0moptimizerD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0moptimizerG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     40\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     41\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL8dy84_G-Ow",
        "colab_type": "text"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Let's write our updates for the different parts.\n",
        "\n",
        "Generally, there is two parts to the optimization:\n",
        "\n",
        "1. Training the discriminator, i.e. maximizing the probability of correctly classifying real vs. fake. We can do this in two parts, first on real images and then on fake images, and accumulate the gradients before conducting an update.\n",
        "2. Training the generator, i.e. minimizing 1 - D(G(z)). The original paper by Goodfellow et. al shows how maximizing D(G(z)) instead seems to provide better gradients. A trick is thus to also maximize here (i.e. not using the 1 minus term but relying on the log(x) part) using the label 1(as if it were a real label) instead of using the 1 minus part with a label of 0. \n",
        "\n",
        "What we will need is to implement this update logic now in seprate steps:\n",
        "\n",
        "1. Forward D with real images\n",
        "2. Calculate loss and gradients of D with real images\n",
        "3. Generate fake images from G\n",
        "4. Forward D with fake images\n",
        "5. Compute loss and gradients of D with fake images\n",
        "6. Sum losses, gradients of fake and real images\n",
        "7. Update weights of D\n",
        "8. Forward all fake images of G through D again\n",
        "9. Calculate loss and gradients for G\n",
        "10. Update G\n",
        "\n",
        "Let us implement these steps in the training loop below. As previously mentioned GANs are difficult to evaluate and we cannot simply use a validation set or similar to evaluate how well our GAN is doing. \n",
        "\n",
        "**It is strongly recommended to start training with 1 epoch only with a GPU runtime to debug before proceeding to train the GAN fully. If you don't see something that resembles (smudgy) faces, there is something wrong with your code. After you have confirmed that your code is working try training for a minimum of 5 epochs (this will take roughly 20 minutes) to see some more compelling results.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2QfvcvWnRHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Loop\n",
        "num_epochs = 5\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        label = torch.full((real_cpu.size(0),), real_label, device=device)\n",
        "        \n",
        "        # Forward pass real batch through D\n",
        "        output = ...\n",
        "        \n",
        "        # Calculate loss on real image mini-batch\n",
        "        errD_real = ...\n",
        "        \n",
        "        # Calculate gradients for D in backward pass\n",
        "        ... # D backward here\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with mini-batch consisting of fake images\n",
        "        # Generate mini-batch of latent sample vectors\n",
        "        # note that we need an extra 1, 1 dimensionality for spatial dimensions\n",
        "        # use torch.randn to draw from a normal distribution.\n",
        "        # don't forget to cast the noise mini-batch to device\n",
        "        noise = ...\n",
        "        \n",
        "        # Generate fake image batch with G\n",
        "        fake = ...\n",
        "        label.fill_(fake_label)\n",
        "        \n",
        "        # Classify all fake batch with D\n",
        "        output = ...\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = ...\n",
        "        # Calculate the gradients for this batch\n",
        "        ... # backward D here \n",
        "        \n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Add the gradients from the all-real and all-fake batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        ... # do the optimizer step on D\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # use trick described in text above: use real labels\n",
        "        \n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = ...\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = ...\n",
        "        # Calculate gradients for G\n",
        "        ... # backward G\n",
        "        \n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        ... # do the optimizer step on G\n",
        "        \n",
        "        # Output training stats\n",
        "        if i % 100 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        \n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "        \n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(torchvision.utils.make_grid(fake, padding=2, normalize=True))\n",
        "            \n",
        "        iters += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMOIMbcwTsw1",
        "colab_type": "text"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Let us visualize the progress of our losses, and our generations from the fixed noise vector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "769BYyeanmCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses, label=\"G\")\n",
        "plt.plot(D_losses, label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph2KIjo6T1G3",
        "colab_type": "text"
      },
      "source": [
        " Progress of fixed latent vector generations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGqwLlbenoQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%capture\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyPQYYlonqcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch[0].to(device)[:64],\n",
        "                                                    padding=5, normalize=True).cpu(),\n",
        "                        (1, 2, 0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1], (1, 2, 0)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}