{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Shakespeare Text Generation with Transformers**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aXr1KaJsXEY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will learn how to implement a simple Transformer model in PyTorch as described in the Veswani et. al 2017 paper: Attention is All You Need. We will learn to generate shakespeare text in similar to the RNN notebook from the previous week. \n"
      ],
      "metadata": {
        "id": "PHfEp63WXqeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# Check whether GPU is available and can be used\n",
        "# if CUDA is found then device is set accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Consider changing your run-time to GPU or training will be slow.\")"
      ],
      "metadata": {
        "id": "5BQ8gXpKXxYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "_pCtPje9j_us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The data: Shakespeare Sonnet"
      ],
      "metadata": {
        "id": "AmAELI6eYSNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shakespeare's sonnets can be found at the following URL featuring all of his works: http://shakespeare.mit.edu/\n",
        "\n",
        "For convenience reasons we have extracted all the plain text of the sonnets: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt into a separate textfile and have added it to the class' repository. We will thus download it from there:"
      ],
      "metadata": {
        "id": "fzWzT5kSYglz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzTinrjWYPZD",
        "outputId": "29345afe-96f1-435e-94d6-345d14768a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-11 13:18:42--  https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94081 (92K) [text/plain]\n",
            "Saving to: ‘sonnets.txt’\n",
            "\n",
            "sonnets.txt         100%[===================>]  91.88K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-04-11 13:18:43 (57.1 MB/s) - ‘sonnets.txt’ saved [94081/94081]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can open the text file and print an excerpt.\n",
        "\n"
      ],
      "metadata": {
        "id": "cH4oI0zXYqdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open shakespeare text file and read the data\n",
        "with open('sonnets.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "# print an excerpt of the text \n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrIpRFgoYuI7",
        "outputId": "2988c1c4-dc3f-4743-b769-6bacfa01d9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  94081\n",
            "From fairest creatures we desire increase,\n",
            "That thereby beauty's rose might never die,\n",
            "But as the ri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a maping from characters to integers"
      ],
      "metadata": {
        "id": "lwMuAzyFZoIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the Character Level Tokenizer\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocabulary_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "s_to_int = { char:i for i,char in enumerate(chars) }\n",
        "int_to_s = { i:char for i,char in enumerate(chars) }\n",
        "lambda_enc = lambda s: [s_to_int[c] for c in s] # A lambda function that takes a string of characters and outputs a list of integers\n",
        "lambda_dec = lambda l: ''.join([int_to_s[i] for i in l]) #A lambda function that takes a list of integers and outputs a string of characters"
      ],
      "metadata": {
        "id": "MYTDaxWHkELI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tranform the text data to integers and split it to train and val sets."
      ],
      "metadata": {
        "id": "7aSnE9afNf5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = torch.tensor(lambda_enc(text), dtype=torch.long) #TODO use the lambda encoder function to transform the text to a Tensor. \n",
        "n = int(0.9*len(data)) #TODO We will split the data 90% for  traininig and the  rest for validation.\n",
        "train_data = data[:n] #TODO\n",
        "val_data = data[n:] #TODO"
      ],
      "metadata": {
        "id": "kOVwceVGkI7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to generate batches for training and validation\n"
      ],
      "metadata": {
        "id": "b78Xy-VgNtwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split='train'):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    indices_x = torch.randint(len(data) - block_size, (batch_size,)) #TODO pick a batch random indices. We recommend using the torch.randint(...,(batch_size,))\n",
        "    input_seqs = torch.stack([data[i:i+block_size] for i in indices_x]) #TODO  Create the input sequences. Hint: You can use use torch.stack function to stack tensors after iterating through indices_x variable\n",
        "    predicted_seqs = torch.stack([data[i+1:i+block_size+1] for i in indices_x]) #TODO Create the sequences that we want to predict. \n",
        "    return input_seqs.to(device), predicted_seqs.to(device)\n"
      ],
      "metadata": {
        "id": "Z3dnMw9xkMiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "rxveDzMfkPVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        #TODO Specify the linear layers for key, query and values. Hint: n_emb refers to the lenght of the inputs for the following layers.\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)  #TODO\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False) #TODO\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False) #TODO\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)# TODO dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # TODO get keys (B,T,C)\n",
        "        q = self.query(x) #TODO get querys (B,T,C)\n",
        "        v = self.value(x) # TODO get the values (B,T,C)\n",
        "\n",
        "        # compute attention scores using the keys and querys\n",
        "        attention_weights = q @ k.transpose(-2,-1) * C**-0.5 # Hint the computation has the following shape (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        attention_weights = attention_weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Hint we obtain a vector of shape (B, T, T)\n",
        "        attention_weights = F.softmax(attention_weights, dim=-1) # TODO softmax computation. Hint: Use the softmax found Functional sublibrary from Pytorch. \n",
        "        attention_weights = self.dropout(attention_weights) #TODO don't foreget the Dropout layer here. \n",
        "        # TODO weighted aggregation of the values. \n",
        "        out = attention_weights @ v # Hint we should have a computation of two vectors of the following shapes: \n",
        "        #(B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "0QJ2CwWlkRyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) #TODO create a module list with multiple heads \n",
        "        self.proj = nn.Linear(n_embd, n_embd) #TODO define a layer to project the outputs of the multiple attention heads. \n",
        "        self.dropout = nn.Dropout(dropout) #TODO don't forget the dropout layer. \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) #TODO compute the the multi-attention heads and concatenate them.\n",
        "        out = self.dropout(self.proj(out)) #TODO compute final output after projection and dropout.\n",
        "        return out"
      ],
      "metadata": {
        "id": "J08Y-Z-bkXxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        ) #TODO define a simple feedforwad network with ReLU and Dropout.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "3ex2t2aLkZ8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.att = MultiHeadAttention(n_head, head_size) #TODO Multi-Head-Attention Block\n",
        "        self.mlp = MLP(n_embd) #TODO MLP block\n",
        "        self.ln1 = nn.LayerNorm(n_embd) #TODO we use the layer normalization before passing the inputs to attention block or MLP block\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.att(self.ln1(x)) #TODO compute output. First use attention and then MLP :)\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Rj15og5fkdzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we bring everything together in a simple Bigram model. \n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocabulary_size, n_embd) #TODO Hint: Use the nn.Embedding function in PyTorch.\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) #TODO Positional Embedding. \n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # Define the number of blocks based on the specified n_layer (number of layers). Hint: Dont forget to wrap the blocks in nn.Sequential.\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # TODO Final layer normalization\n",
        "        self.lm_head = nn.Linear(n_embd, vocabulary_size) #TODO Linear layer for the output generation.\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # index and targets are both (B,T) tensor of integers\n",
        "        #TODO compute Token and positional embeddings.\n",
        "        tok_emb = self.token_embedding_table(idx) #Hint: Tensor shape should be (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # Hint: Tensor shape should be (T,C)\n",
        "        x = tok_emb + pos_emb # TODO sum token and position embeddings. Hint: Tensor shape should be  (B,T,C)\n",
        "        x = self.blocks(x) # TODO pass the input to the Attention-Blocks  Hint: Tensor shape should be (B,T,C)\n",
        "        x = self.ln_f(x) # TODO Normalization \n",
        "        logits = self.lm_head(x) # TODO Compute logits. Hint: The final Tensor shape should be (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) #TODO Reshape the logits and the targets.\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets) #TODO Cross entropy loss\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, context_indices, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = context_indices[:, -block_size:] # TODO crop context_indices to match our defined block_size\n",
        "            # TODO get the predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] # TODO  Transform logists to (B, C). Hint: We only care about thelast time step\n",
        "            probs = F.softmax(logits, dim=-1) # TODO get probabilities from the logits\n",
        "\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # TODO sample from the obtained distribution. Hint: use Torch.multinomial function. \n",
        "            context_indices = torch.cat((context_indices, idx_next), dim=1) #TODO append sampled index to the running sequence.\n",
        "        return context_indices #We should have a tensor with shape (B, T+1)\n"
      ],
      "metadata": {
        "id": "8Z4PGZtukhQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "#TODO create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLp_H9ofkme9",
        "outputId": "b3bb7cc3-b761-4623-cf3f-3835f22aba95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209213 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters=10000\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # Evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # TODO Training loop\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6UPrKHxaw2u",
        "outputId": "d846d366-0372-4af6-ff5b-d2ab81217054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3294, val loss 4.3360\n",
            "step 100: train loss 2.4908, val loss 2.5141\n",
            "step 200: train loss 2.3485, val loss 2.3625\n",
            "step 300: train loss 2.2509, val loss 2.2695\n",
            "step 400: train loss 2.1984, val loss 2.2258\n",
            "step 500: train loss 2.1264, val loss 2.1631\n",
            "step 600: train loss 2.0778, val loss 2.1177\n",
            "step 700: train loss 2.0247, val loss 2.0807\n",
            "step 800: train loss 1.9798, val loss 2.0485\n",
            "step 900: train loss 1.9410, val loss 2.0328\n",
            "step 1000: train loss 1.9165, val loss 1.9922\n",
            "step 1100: train loss 1.8724, val loss 1.9618\n",
            "step 1200: train loss 1.8446, val loss 1.9447\n",
            "step 1300: train loss 1.8190, val loss 1.9079\n",
            "step 1400: train loss 1.8029, val loss 1.9127\n",
            "step 1500: train loss 1.7745, val loss 1.8965\n",
            "step 1600: train loss 1.7674, val loss 1.8670\n",
            "step 1700: train loss 1.7408, val loss 1.8612\n",
            "step 1800: train loss 1.7305, val loss 1.8548\n",
            "step 1900: train loss 1.7255, val loss 1.8554\n",
            "step 2000: train loss 1.7055, val loss 1.8343\n",
            "step 2100: train loss 1.6896, val loss 1.8330\n",
            "step 2200: train loss 1.6809, val loss 1.8302\n",
            "step 2300: train loss 1.6564, val loss 1.8228\n",
            "step 2400: train loss 1.6557, val loss 1.8081\n",
            "step 2500: train loss 1.6385, val loss 1.8108\n",
            "step 2600: train loss 1.6271, val loss 1.8094\n",
            "step 2700: train loss 1.6147, val loss 1.8023\n",
            "step 2800: train loss 1.6129, val loss 1.7969\n",
            "step 2900: train loss 1.5918, val loss 1.7912\n",
            "step 3000: train loss 1.5889, val loss 1.7668\n",
            "step 3100: train loss 1.5922, val loss 1.7872\n",
            "step 3200: train loss 1.5695, val loss 1.7775\n",
            "step 3300: train loss 1.5742, val loss 1.7646\n",
            "step 3400: train loss 1.5568, val loss 1.7795\n",
            "step 3500: train loss 1.5512, val loss 1.7623\n",
            "step 3600: train loss 1.5405, val loss 1.7740\n",
            "step 3700: train loss 1.5334, val loss 1.7709\n",
            "step 3800: train loss 1.5383, val loss 1.7788\n",
            "step 3900: train loss 1.5311, val loss 1.7737\n",
            "step 4000: train loss 1.5106, val loss 1.7803\n",
            "step 4100: train loss 1.5036, val loss 1.7467\n",
            "step 4200: train loss 1.5035, val loss 1.7577\n",
            "step 4300: train loss 1.4993, val loss 1.7592\n",
            "step 4400: train loss 1.4865, val loss 1.7603\n",
            "step 4500: train loss 1.4792, val loss 1.7625\n",
            "step 4600: train loss 1.4701, val loss 1.7609\n",
            "step 4700: train loss 1.4662, val loss 1.7632\n",
            "step 4800: train loss 1.4672, val loss 1.7656\n",
            "step 4900: train loss 1.4442, val loss 1.7621\n",
            "step 5000: train loss 1.4444, val loss 1.7588\n",
            "step 5100: train loss 1.4410, val loss 1.7499\n",
            "step 5200: train loss 1.4324, val loss 1.7785\n",
            "step 5300: train loss 1.4370, val loss 1.7684\n",
            "step 5400: train loss 1.4390, val loss 1.7772\n",
            "step 5500: train loss 1.4184, val loss 1.7886\n",
            "step 5600: train loss 1.4192, val loss 1.7697\n",
            "step 5700: train loss 1.4010, val loss 1.7563\n",
            "step 5800: train loss 1.4110, val loss 1.7782\n",
            "step 5900: train loss 1.3936, val loss 1.7713\n",
            "step 6000: train loss 1.3933, val loss 1.7798\n",
            "step 6100: train loss 1.3966, val loss 1.7821\n",
            "step 6200: train loss 1.3893, val loss 1.7808\n",
            "step 6300: train loss 1.3886, val loss 1.7804\n",
            "step 6400: train loss 1.3711, val loss 1.7848\n",
            "step 6500: train loss 1.3753, val loss 1.7931\n",
            "step 6600: train loss 1.3666, val loss 1.8043\n",
            "step 6700: train loss 1.3579, val loss 1.7837\n",
            "step 6800: train loss 1.3593, val loss 1.7958\n",
            "step 6900: train loss 1.3595, val loss 1.7883\n",
            "step 7000: train loss 1.3520, val loss 1.7940\n",
            "step 7100: train loss 1.3564, val loss 1.7951\n",
            "step 7200: train loss 1.3400, val loss 1.8023\n",
            "step 7300: train loss 1.3332, val loss 1.7998\n",
            "step 7400: train loss 1.3354, val loss 1.8251\n",
            "step 7500: train loss 1.3233, val loss 1.7918\n",
            "step 7600: train loss 1.3232, val loss 1.8166\n",
            "step 7700: train loss 1.3277, val loss 1.8172\n",
            "step 7800: train loss 1.3249, val loss 1.8268\n",
            "step 7900: train loss 1.3145, val loss 1.8283\n",
            "step 8000: train loss 1.2990, val loss 1.8297\n",
            "step 8100: train loss 1.3068, val loss 1.8324\n",
            "step 8200: train loss 1.2920, val loss 1.8178\n",
            "step 8300: train loss 1.2933, val loss 1.8448\n",
            "step 8400: train loss 1.2859, val loss 1.8591\n",
            "step 8500: train loss 1.2894, val loss 1.8523\n",
            "step 8600: train loss 1.2798, val loss 1.8372\n",
            "step 8700: train loss 1.2806, val loss 1.8406\n",
            "step 8800: train loss 1.2710, val loss 1.8525\n",
            "step 8900: train loss 1.2738, val loss 1.8660\n",
            "step 9000: train loss 1.2661, val loss 1.8481\n",
            "step 9100: train loss 1.2609, val loss 1.8725\n",
            "step 9200: train loss 1.2564, val loss 1.8634\n",
            "step 9300: train loss 1.2487, val loss 1.8749\n",
            "step 9400: train loss 1.2687, val loss 1.9028\n",
            "step 9500: train loss 1.2432, val loss 1.8860\n",
            "step 9600: train loss 1.2325, val loss 1.8668\n",
            "step 9700: train loss 1.2374, val loss 1.8644\n",
            "step 9800: train loss 1.2377, val loss 1.8897\n",
            "step 9900: train loss 1.2322, val loss 1.8992\n",
            "step 9999: train loss 1.2370, val loss 1.9207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(lambda_dec(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "kNHHueSxkrPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1c2944-d22b-4064-9657-44e5d7179dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "That I am old in you doth view\n",
            "For men of thy care, mysicide,'\n",
            "Than before ould that which woman whence case,\n",
            "And Event Time's both outgo;\n",
            "Under thee and my sight burreing may eyous,\n",
            "Thereforoul's self willeds what I sometime removed,\n",
            "In of you earers hot offence.'\n",
            "Of thou forth my sightdered of me,\n",
            "Some of you and your stand\n",
            "That mayst fazed ful-I may them' untrue,\n",
            "Were me pays shows now her suse,\n",
            "But why thou unseasure his commentate of seeith forner sock hence.\n",
            "Hid doth to be present are earth strong made eye?\n",
            "From what not the raccely makind cuments youth and use so so cold;\n",
            "Tell saucy all a so set as to not fears,\n",
            "To maturz elsend to me.'s but one is thought,\n",
            "That is mind, excuse my self, and for heaven's besire!\n",
            "Ah! and mine oceang bear.\n",
            "\n",
            "When If thou maysumulatest dead;\n",
            "To sow, thy fair redged\n",
            "Thy beauty, unksomed more shame,\n",
            "And kill no longer of you my sight?\n",
            "Returns are the conquest, plays,\n",
            "The errs'mage sight sick thousa'd you,\n",
            "More and in him that cannot spass would another of my nerfeck of as his stol'n prarthless me for cross.\n",
            "\n",
            "Sometion sweets death-rank swould calfil one,\n",
            "For to in the treachse, sweets writesterefullues strive,\n",
            "Tan flow to when I am nay.\n",
            "\n",
            "So writesme of thy natulty give;\n",
            "For kendon the thinkings that sinkles,\n",
            "And that true life, love to his bood;\n",
            "And so all the summer's lines of nearing.\n",
            "His of thee will excel.\n",
            "Yet love, my love received thy cannot boast,\n",
            "Shall may in hounders see to that my sauch is be rich,\n",
            "Richer that I am folld in my seeing absence:\n",
            "Then of youth\n",
            "The erry jects me! thetereby?\n",
            ") non! not voind those invention of him state,\n",
            "And for the be makes sight, thou thy confined from thy thougayst yourself your check,\n",
            "To all forth did gives them must be,\n",
            "Thou rides that now thy founders some insconstor,\n",
            "To set that take a deter almosther's in his scythiee:\n",
            "And yet is an made of firettered,\n",
            "And something me solity; if for than which a his break in of your rettengen all tweetest 'tis fair from my verse accemary\n",
            "Time ners to hid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the results obtained this week and the results obtained using RNNs and LSTMS from last week's practical assignement. Do Transformer models signficantly outperfroms their RNN counterparts? If not, what are the missing components to achieve the impressive text generation performance that Transformers are known for?  "
      ],
      "metadata": {
        "id": "mBPHZx2rikWq"
      }
    }
  ]
}